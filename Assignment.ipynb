{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>4173</td>\n",
       "      <td>0</td>\n",
       "      <td>yeah!!! this made my day! thanks @user   #win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8347</th>\n",
       "      <td>8348</td>\n",
       "      <td>0</td>\n",
       "      <td>i like your smile :)  bogumday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30456</th>\n",
       "      <td>30457</td>\n",
       "      <td>0</td>\n",
       "      <td>i didnt know @user was still around</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31565</th>\n",
       "      <td>31566</td>\n",
       "      <td>0</td>\n",
       "      <td>@user ahhhh then let the madness unravel   ð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23376</th>\n",
       "      <td>23377</td>\n",
       "      <td>0</td>\n",
       "      <td>had a curry rice at airside cafeðâ¤ï¸ #lu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet\n",
       "4172    4173      0     yeah!!! this made my day! thanks @user   #win \n",
       "8347    8348      0                   i like your smile :)  bogumday  \n",
       "30456  30457      0              i didnt know @user was still around  \n",
       "31565  31566      0  @user ahhhh then let the madness unravel   ð...\n",
       "23376  23377      0  had a curry rice at airside cafeðâ¤ï¸ #lu..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the training dataset\n",
    "DATASET_COLUMNS=['id','label','tweet']\n",
    "df_train = pd.read_csv('https://raw.githubusercontent.com/prateekjoshi565/twitter_sentiment_analysis/master/train_E6oV3lV.csv', names=DATASET_COLUMNS, header=0)\n",
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train data is 31962\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31962.000000</td>\n",
       "      <td>31962.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15981.500000</td>\n",
       "      <td>0.070146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9226.778988</td>\n",
       "      <td>0.255397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7991.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15981.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23971.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31962.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         label\n",
       "count  31962.000000  31962.000000\n",
       "mean   15981.500000      0.070146\n",
       "std     9226.778988      0.255397\n",
       "min        1.000000      0.000000\n",
       "25%     7991.250000      0.000000\n",
       "50%    15981.500000      0.000000\n",
       "75%    23971.750000      0.000000\n",
       "max    31962.000000      1.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Length of the dataset\n",
    "\n",
    "print('length of train data is', len(df_train))\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11181</th>\n",
       "      <td>43144</td>\n",
       "      <td>#cuteecho   with both #grandmas ( #harmony too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>41944</td>\n",
       "      <td>can #lighttherapy help with   or #depression? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10018</th>\n",
       "      <td>41981</td>\n",
       "      <td>@user the 4th remake of hi-5 doesn't have the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9320</th>\n",
       "      <td>41283</td>\n",
       "      <td>and we're in the game!!! installed in minutes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>32046</td>\n",
       "      <td>and the forecast looks good for the weather al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet\n",
       "11181  43144  #cuteecho   with both #grandmas ( #harmony too...\n",
       "9981   41944  can #lighttherapy help with   or #depression? ...\n",
       "10018  41981  @user the 4th remake of hi-5 doesn't have the ...\n",
       "9320   41283  and we're in the game!!! installed in minutes ...\n",
       "83     32046  and the forecast looks good for the weather al..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Testing dataset\n",
    "DATASET_COLUMNS=['id','tweet']\n",
    "df_test = pd.read_csv('https://raw.githubusercontent.com/prateekjoshi565/twitter_sentiment_analysis/master/test_tweets_anuFYb8.csv', names=DATASET_COLUMNS,header=0)\n",
    "df_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of test data is 17197\n"
     ]
    }
   ],
   "source": [
    "#Length of the dataset\n",
    "\n",
    "print('length of test data is', len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the number of target values\n",
    "df_train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the distribution for dataset.\n",
    "sns.countplot(x='label', data=df_train)\n",
    "# Storing data in lists.\n",
    "text, sentiment = list(df_train['tweet']), list(df_train['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31957    ate @user isz that youuu?ðððððð...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31960    @user #sikh #temple vandalised in in #calgary,...\n",
       "31961                     thank you @user for you follow  \n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train['tweet']=df_train['tweet'].str.lower()\n",
    "df_train['tweet'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @user father dysfunctional selfish drags kids ...\n",
       "1    @user @user thanks #lyft credit can't use caus...\n",
       "2                                       bihday majesty\n",
       "3    #model love u take u time urð±!!! ððð...\n",
       "4                      factsguide: society #motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Stopwords\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "def cleaning_stopwords(text,STOPWORDS):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda text: cleaning_stopwords(text,STOPWORDS))\n",
    "df_train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31957    ate user isz youuuððððððð...\n",
       "31958    see nina turner airwaves trying wrap mantle ge...\n",
       "31959      listening sad songs monday morning otw work sad\n",
       "31960    user sikh temple vandalised calgary wso condem...\n",
       "31961                                    thank user follow\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing Punctuations\n",
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "df_train['tweet']= df_train['tweet'].apply(lambda x: cleaning_punctuations(x))\n",
    "df_train['tweet'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31957    ate user isz youuuððððððð...\n",
       "31958    see nina turner airwaves trying wrap mantle ge...\n",
       "31959      listening sad songs monday morning otw work sad\n",
       "31960    user sikh temple vandalised calgary wso condem...\n",
       "31961                                    thank user follow\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing repeating characters\n",
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: cleaning_repeating_char(x))\n",
    "df_train['tweet'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31957    ate user isz youuuððððððð...\n",
       "31958    see nina turner airwaves trying wrap mantle ge...\n",
       "31959      listening sad songs monday morning otw work sad\n",
       "31960    user sikh temple vandalised calgary wso condem...\n",
       "31961                                    thank user follow\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing URL’s\n",
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: cleaning_URLs(x))\n",
    "df_train['tweet'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31957    ate user isz youuuððððððð...\n",
       "31958    see nina turner airwaves trying wrap mantle ge...\n",
       "31959      listening sad songs monday morning otw work sad\n",
       "31960    user sikh temple vandalised calgary wso condem...\n",
       "31961                                    thank user follow\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing Numeric numbers\n",
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: cleaning_numbers(x))\n",
    "df_train['tweet'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31712/173152707.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\regexp.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;31m# If our regexp matches tokens, use re.findall:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_regexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'w+')\n",
    "df_train['tweet'] = df_train['tweet'].apply(tokenizer.tokenize)\n",
    "df_train['tweet'].head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73e03da126b73bfff3642ec5261d56fa25c444ea595de51041687efaa60dda41"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
